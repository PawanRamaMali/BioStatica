{
  "hash": "f049e13e3d857319a137ccf809df43f0",
  "result": {
    "engine": "knitr",
    "markdown": "# Interrelationships & Modeling\n\nMove beyond simple group comparisons to understand relationships between variables and build predictive models.\n\n## Introduction\n\nIn biostatistics, we often need to understand relationships between variables and make predictions. This chapter covers fundamental techniques for exploring associations, building predictive models, and evaluating their performance in medical and biological contexts.\n\n## Correlation Analysis\n\n### Understanding Correlation\n\nCorrelation measures the strength and direction of a linear relationship between two continuous variables. In biostatistics, we might examine correlations between:\n\n- Blood pressure and age\n- BMI and cholesterol levels\n- Gene expression levels between related genes\n\n### Pearson Correlation Coefficient\n\nThe Pearson correlation coefficient ($r$) ranges from -1 to +1:\n\n$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n\nWhere:\n- $r = 1$: Perfect positive linear relationship\n- $r = 0$: No linear relationship\n- $r = -1$: Perfect negative linear relationship\n\n::: {.callout-tip}\n## Interpreting Correlation Strength\n- |r| < 0.3: Weak correlation\n- 0.3 ≤ |r| < 0.7: Moderate correlation\n- |r| ≥ 0.7: Strong correlation\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Correlation between age and blood pressure\nlibrary(ggplot2)\n\n# Simulate data\nset.seed(123)\nn <- 100\nage <- rnorm(n, mean = 45, sd = 15)\nbp_systolic <- 90 + 0.8 * age + rnorm(n, sd = 10)\n\n# Calculate correlation\ncor_result <- cor.test(age, bp_systolic)\nprint(paste(\"Correlation:\", round(cor_result$estimate, 3)))\nprint(paste(\"P-value:\", round(cor_result$p.value, 4)))\n\n# Visualize\nggplot(data.frame(age, bp_systolic), aes(x = age, y = bp_systolic)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Age vs Systolic Blood Pressure\",\n       x = \"Age (years)\", y = \"Systolic BP (mmHg)\") +\n  theme_minimal()\n```\n:::\n\n\n\n\n\n\n## Simple Linear Regression\n\n### The Linear Model\n\nSimple linear regression models the relationship between a predictor variable (X) and an outcome variable (Y):\n\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\n\nWhere:\n- $\\beta_0$: Intercept (value of Y when X = 0)\n- $\\beta_1$: Slope (change in Y per unit change in X)\n- $\\epsilon_i$: Random error term\n\n### Example: Predicting Blood Pressure from Age\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression model\nmodel <- lm(bp_systolic ~ age)\nsummary(model)\n\n# The model: bp_systolic = β₀ + β₁ × age\n# Interpretation: For each additional year of age,\n# systolic BP increases by β₁ mmHg on average\n```\n:::\n\n\n\n\n\n\n## Logistic Regression\n\n### When to Use Logistic Regression\n\nWhen your outcome variable is binary (diseased/healthy, survived/died, positive/negative test), logistic regression is the appropriate method.\n\n### The Logistic Model\n\nInstead of predicting the outcome directly, logistic regression predicts the log-odds:\n\n$$\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_k X_k$$\n\nWhere $p$ is the probability of the event occurring.\n\nThe probability is then:\n\n$$p = \\frac{e^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k}}{1 + e^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k}}$$\n\n### Example: Predicting Heart Disease Risk\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate heart disease data\nset.seed(456)\nn <- 500\nage <- rnorm(n, 55, 12)\ncholesterol <- rnorm(n, 200, 40)\nsmoking <- rbinom(n, 1, 0.3)\n\n# Create log-odds for heart disease\nlog_odds <- -5 + 0.05*age + 0.01*cholesterol + 1.2*smoking\nprob_disease <- exp(log_odds)/(1 + exp(log_odds))\nheart_disease <- rbinom(n, 1, prob_disease)\n\n# Fit logistic regression\nlogit_model <- glm(heart_disease ~ age + cholesterol + smoking,\n                   family = binomial)\nsummary(logit_model)\n\n# Interpretation: exp(coefficient) gives the odds ratio\nexp(coef(logit_model))\n```\n:::\n\n\n\n\n\n\n## Model Evaluation for Classification\n\n### The Confusion Matrix\n\nA confusion matrix is a table that shows how well our classification model performs. It's like a report card for prediction models.\n\n::: {.callout-note}\n## Think of it this way\nImagine you're a doctor using a test to diagnose disease. The confusion matrix shows:\n- How many sick patients you correctly identified (True Positives)\n- How many healthy patients you correctly identified (True Negatives)\n- How many times you missed the disease (False Negatives)\n- How many times you incorrectly diagnosed disease (False Positives)\n:::\n\n### 2×2 Confusion Matrix\n\n|                | **Predicted** |           |\n|----------------|:-------------:|:---------:|\n| **Actual**     | **Positive**  | **Negative** |\n| **Positive**   | TP            | FN        |\n| **Negative**   | FP            | TN        |\n\nWhere:\n- **TP (True Positives)**: Correctly predicted positive cases\n- **TN (True Negatives)**: Correctly predicted negative cases\n- **FP (False Positives)**: Incorrectly predicted positive (Type I error)\n- **FN (False Negatives)**: Incorrectly predicted negative (Type II error)\n\n### Example: COVID-19 Test Evaluation\n\nLet's say we have a COVID-19 test and we tested 1000 people:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create confusion matrix data\nlibrary(caret)\n\n# Simulate COVID test results\nset.seed(789)\nn <- 1000\ntrue_status <- c(rep(\"Positive\", 100), rep(\"Negative\", 900))  # 10% prevalence\ntest_sensitivity <- 0.95  # 95% of positive cases detected\ntest_specificity <- 0.98  # 98% of negative cases correctly identified\n\npredicted_status <- ifelse(\n  true_status == \"Positive\",\n  ifelse(runif(sum(true_status == \"Positive\")) < test_sensitivity, \"Positive\", \"Negative\"),\n  ifelse(runif(sum(true_status == \"Negative\")) < test_specificity, \"Negative\", \"Positive\")\n)\n\n# Create confusion matrix\ncm <- confusionMatrix(as.factor(predicted_status), as.factor(true_status), positive = \"Positive\")\nprint(cm)\n\n# Visualize confusion matrix\nlibrary(ggplot2)\ncm_table <- as.data.frame(cm$table)\nggplot(cm_table, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 12, color = \"white\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  labs(title = \"COVID-19 Test Confusion Matrix\",\n       x = \"Actual Status\", y = \"Predicted Status\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n```\n:::\n\n\n\n\n\n\n## Diagnostic Test Performance Metrics\n\nFrom the confusion matrix, we can calculate several important metrics:\n\n### Sensitivity (True Positive Rate)\n\n**What it means**: Of all the people who actually have the disease, what percentage does our test catch?\n\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n\n**Example**: If sensitivity = 0.95, then our test catches 95% of COVID-positive patients.\n\n### Specificity (True Negative Rate)\n\n**What it means**: Of all the people who don't have the disease, what percentage does our test correctly identify as negative?\n\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n\n**Example**: If specificity = 0.98, then our test correctly identifies 98% of COVID-negative patients.\n\n### Positive Predictive Value (PPV)\n\n**What it means**: If the test is positive, what's the probability the person actually has the disease?\n\n$$\\text{PPV} = \\frac{TP}{TP + FP}$$\n\n### Negative Predictive Value (NPV)\n\n**What it means**: If the test is negative, what's the probability the person actually doesn't have the disease?\n\n$$\\text{NPV} = \\frac{TN}{TN + FN}$$\n\n::: {.callout-warning}\n## Important: PPV and NPV depend on disease prevalence!\nIn a population where disease is rare, even a good test will have many false positives, lowering PPV. In a high-prevalence population, the same test will have better PPV.\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate metrics manually\ncalculate_metrics <- function(tp, tn, fp, fn) {\n  sensitivity <- tp / (tp + fn)\n  specificity <- tn / (tn + fp)\n  ppv <- tp / (tp + fp)\n  npv <- tn / (tn + fn)\n  accuracy <- (tp + tn) / (tp + tn + fp + fn)\n\n  return(list(\n    Sensitivity = sensitivity,\n    Specificity = specificity,\n    PPV = ppv,\n    NPV = npv,\n    Accuracy = accuracy\n  ))\n}\n\n# Example with our COVID data\nmetrics <- calculate_metrics(tp = 95, tn = 882, fp = 18, fn = 5)\nprint(metrics)\n```\n:::\n\n\n\n\n\n\n## ROC Curves and AUC\n\n### What is an ROC Curve?\n\nROC stands for \"Receiver Operating Characteristic.\" Think of it as a way to visualize the trade-off between catching true cases (sensitivity) and avoiding false alarms (1 - specificity).\n\n**Real-world analogy**: Imagine you're a security guard with a metal detector. You can adjust the sensitivity:\n- High sensitivity: Catches all weapons but sets off many false alarms\n- Low sensitivity: Fewer false alarms but might miss some weapons\n\nThe ROC curve shows this trade-off at different threshold settings.\n\n### Mathematical Definition\n\nThe ROC curve plots:\n- **Y-axis**: True Positive Rate (Sensitivity) = $\\frac{TP}{TP + FN}$\n- **X-axis**: False Positive Rate (1 - Specificity) = $\\frac{FP}{FP + TN}$\n\n### Creating ROC Curves\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\nlibrary(ggplot2)\n\n# Simulate biomarker data for disease diagnosis\nset.seed(101)\nn_healthy <- 500\nn_diseased <- 200\n\n# Healthy individuals: lower biomarker levels\nhealthy_biomarker <- rnorm(n_healthy, mean = 10, sd = 3)\n\n# Diseased individuals: higher biomarker levels\ndiseased_biomarker <- rnorm(n_diseased, mean = 15, sd = 3)\n\n# Combine data\nbiomarker_values <- c(healthy_biomarker, diseased_biomarker)\ntrue_status <- c(rep(0, n_healthy), rep(1, n_diseased))\n\n# Create ROC curve\nroc_obj <- roc(true_status, biomarker_values)\n\n# Plot ROC curve\nggroc(roc_obj, color = \"blue\", size = 1) +\n  geom_abline(intercept = 1, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"ROC Curve for Biomarker Test\",\n       x = \"False Positive Rate (1 - Specificity)\",\n       y = \"True Positive Rate (Sensitivity)\") +\n  theme_minimal() +\n  annotate(\"text\", x = 0.7, y = 0.3,\n           label = paste(\"AUC =\", round(auc(roc_obj), 3)), size = 5)\n```\n:::\n\n\n\n\n\n\n### Understanding AUC (Area Under the Curve)\n\nThe AUC summarizes the ROC curve into a single number between 0 and 1:\n\n$$\\text{AUC} = \\int_0^1 \\text{TPR}(t) \\, d(\\text{FPR}(t))$$\n\n**Interpretation**:\n- **AUC = 0.5**: Random guessing (coin flip)\n- **AUC = 0.7**: Acceptable discrimination\n- **AUC = 0.8**: Excellent discrimination\n- **AUC = 0.9**: Outstanding discrimination\n- **AUC = 1.0**: Perfect discrimination\n\n::: {.callout-tip}\n## Intuitive AUC Interpretation\nAUC represents the probability that a randomly selected positive case will have a higher predicted probability than a randomly selected negative case.\n\nIf AUC = 0.8, then 80% of the time, a diseased person will have a higher biomarker value than a healthy person.\n:::\n\n### Comparing Multiple Tests\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate three different biomarkers\nbiomarker1 <- c(rnorm(n_healthy, 10, 3), rnorm(n_diseased, 15, 3))  # Good test\nbiomarker2 <- c(rnorm(n_healthy, 12, 4), rnorm(n_diseased, 14, 4))  # Moderate test\nbiomarker3 <- c(rnorm(n_healthy, 11, 5), rnorm(n_diseased, 13, 5))  # Poor test\n\n# Create ROC curves\nroc1 <- roc(true_status, biomarker1)\nroc2 <- roc(true_status, biomarker2)\nroc3 <- roc(true_status, biomarker3)\n\n# Plot comparison\nlibrary(patchwork)\np1 <- ggroc(list(\"Biomarker 1\" = roc1, \"Biomarker 2\" = roc2, \"Biomarker 3\" = roc3)) +\n  scale_color_manual(values = c(\"blue\", \"green\", \"red\")) +\n  geom_abline(intercept = 1, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Comparison of Three Biomarkers\",\n       x = \"False Positive Rate\", y = \"True Positive Rate\") +\n  theme_minimal()\n\n# Print AUC values\ncat(\"AUC Values:\\n\")\ncat(\"Biomarker 1:\", round(auc(roc1), 3), \"\\n\")\ncat(\"Biomarker 2:\", round(auc(roc2), 3), \"\\n\")\ncat(\"Biomarker 3:\", round(auc(roc3), 3), \"\\n\")\n\nprint(p1)\n```\n:::\n\n\n\n\n\n\n### Optimal Threshold Selection\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find optimal threshold using Youden's J statistic\ncoords_all <- coords(roc1, \"all\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\ncoords_all$youden <- coords_all$sensitivity + coords_all$specificity - 1\noptimal_threshold <- coords_all[which.max(coords_all$youden), ]\n\ncat(\"Optimal Threshold Analysis:\\n\")\ncat(\"Threshold:\", round(optimal_threshold$threshold, 2), \"\\n\")\ncat(\"Sensitivity:\", round(optimal_threshold$sensitivity, 3), \"\\n\")\ncat(\"Specificity:\", round(optimal_threshold$specificity, 3), \"\\n\")\ncat(\"Youden's J:\", round(optimal_threshold$youden, 3), \"\\n\")\n\n# Plot threshold selection\nthreshold_plot <- ggplot(coords_all, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1) +\n  geom_vline(xintercept = optimal_threshold$threshold, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  labs(title = \"Sensitivity and Specificity vs Threshold\",\n       x = \"Biomarker Threshold\", y = \"Rate\", color = \"Metric\") +\n  theme_minimal()\n\nprint(threshold_plot)\n```\n:::\n\n\n\n\n\n\n## Practical Applications in Biostatistics\n\n### 1. Biomarker Validation\n\nWhen developing diagnostic biomarkers:\n- Use ROC analysis to assess discriminative ability\n- Calculate AUC to compare different biomarkers\n- Determine optimal cutoff points balancing sensitivity and specificity\n\n### 2. Risk Prediction Models\n\nIn clinical risk assessment:\n- Logistic regression to model disease probability\n- Confusion matrices to evaluate model performance\n- ROC curves to assess calibration across risk thresholds\n\n### 3. Treatment Response Prediction\n\nFor personalized medicine:\n- Classification models to predict treatment response\n- Performance metrics to validate model utility\n- Threshold optimization for clinical decision-making\n\n::: {.callout-note}\n## Key Takeaways\n\n1. **Confusion matrices** provide a complete picture of classification performance\n2. **Sensitivity and specificity** are fundamental test characteristics\n3. **PPV and NPV** depend on disease prevalence in your population\n4. **ROC curves** visualize the sensitivity-specificity trade-off\n5. **AUC** provides a single metric for overall discriminative ability\n6. **Threshold selection** should consider clinical consequences of false positives vs false negatives\n:::\n\n## Python Implementation\n\nFor those preferring Python, here's equivalent code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n# Simulate data\nnp.random.seed(123)\nn = 1000\nage = np.random.normal(55, 12, n)\ncholesterol = np.random.normal(200, 40, n)\nsmoking = np.random.binomial(1, 0.3, n)\n\n# Create outcome\nlog_odds = -5 + 0.05*age + 0.01*cholesterol + 1.2*smoking\nprob_disease = np.exp(log_odds)/(1 + np.exp(log_odds))\nheart_disease = np.random.binomial(1, prob_disease, n)\n\n# Fit logistic regression\nX = np.column_stack([age, cholesterol, smoking])\nmodel = LogisticRegression()\nmodel.fit(X, heart_disease)\n\n# Predictions\ny_pred_proba = model.predict_proba(X)[:, 1]\ny_pred = model.predict(X)\n\n# Confusion matrix\ncm = confusion_matrix(heart_disease, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(heart_disease, y_pred_proba)\nauc_score = roc_auc_score(heart_disease, y_pred_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc_score:.3f})')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n\n# Classification report\nprint(classification_report(heart_disease, y_pred))\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}